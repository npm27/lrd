shiny::runApp()
runApp()
rsconnect::deployApp('C:/Users/nickm.000/Documents/GitHub/lrd/lrdSHINY')
rsconnect::deployApp('C:/Users/nickm.000/Documents/GitHub/lrd/lrdSHINY')
setwd("~/GitHub/lrd/Manuscript")
setwd("~/GitHub/lrd/Test Data/Rescored")
setwd("~/GitHub/lrd/Test Data/Original")
####set up####
dat = read.csv("Maxwell Buchanan 2019 part 2.csv")
colnames(dat)[3] = "Cue"
colnames(dat)[6] = "manually_coded"
library(lrd)
#make sure response and key are each lowercase
dat$Response = tolower(dat$Response)
dat$Key = tolower(dat$Key)
#Now compute the percent match
maxbuch_matched = percent_match(dat$Response, key = dat$Key, id = dat$Sub.ID)
####Score the data####
#going to use 10 cutoff points: 100, 95, 90, 85, 80, 75, 70, 65, 60, 55, 50
#first add percent match
dat$percent_match = maxbuch_matched$percent_match
#100%
score_recall(maxbuch_matched, set.cutoff = 1)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$one_hundred = output$scored
#95%
score_recall(maxbuch_matched, set.cutoff = .95)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$ninety_five = output$scored
#90%
score_recall(maxbuch_matched, set.cutoff = .90)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$ninety= output$scored
#85%
score_recall(maxbuch_matched, set.cutoff = .85)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$eightyfive= output$scored
#80%
score_recall(maxbuch_matched, set.cutoff = .80)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$eighty= output$scored
#75%
score_recall(maxbuch_matched, set.cutoff = .75)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$seventy_five = output$scored
#70%
score_recall(maxbuch_matched, set.cutoff = .70)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$seventy = output$scored
#65%
score_recall(maxbuch_matched, set.cutoff = .65)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$sixty_five = output$scored
#60%
score_recall(maxbuch_matched, set.cutoff = .60)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$sixty = output$scored
#55%
score_recall(maxbuch_matched, set.cutoff = .55)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$fifty_five = output$scored
#50%
score_recall(maxbuch_matched, set.cutoff = .50)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$fifty = output$scored
#write.csv(dat,
#      file = "max_buch processed.csv", row.names = F)
write.csv(dat,
file = "max_buch processed.csv", row.names = F)
####set up####
dat = read.csv("Maxwell_Huff output.csv")
colnames(dat)[3] = "Cue"
colnames(dat)[4] = "Key"
colnames(dat)[2] = "Sub.ID"
colnames(dat)[6] = "manually_coded"
library(lrd)
#make sure response and key are each lowercase
dat$Response = tolower(dat$Response)
dat$Key = tolower(dat$Key)
#Now compute the percent match
maxhuff_matched = percent_match(dat$Response, key = dat$Key, id = dat$Sub.ID)
####Score the data####
#going to use 10 cutoff points: 100, 95, 90, 85, 80, 75, 70, 65, 60, 55, 50
#first add percent match
dat$percent_match = maxhuff_matched$percent_match
#100%
score_recall(maxhuff_matched, set.cutoff = 1)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$one_hundred = output$scored
#95%
score_recall(maxhuff_matched, set.cutoff = .95)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$ninety_five = output$scored
#90%
score_recall(maxhuff_matched, set.cutoff = .90)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$ninety= output$scored
#85%
score_recall(maxhuff_matched, set.cutoff = .85)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$eightyfive= output$scored
#80%
score_recall(maxhuff_matched, set.cutoff = .80)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$eighty= output$scored
#75%
score_recall(maxhuff_matched, set.cutoff = .75)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$seventy_five = output$scored
#70%
score_recall(maxhuff_matched, set.cutoff = .70)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$seventy = output$scored
#65%
score_recall(maxhuff_matched, set.cutoff = .65)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$sixty_five = output$scored
#60%
score_recall(maxhuff_matched, set.cutoff = .60)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$sixty = output$scored
#55%
score_recall(maxhuff_matched, set.cutoff = .55)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$fifty_five = output$scored
#50%
score_recall(maxhuff_matched, set.cutoff = .50)
#now add the scored data to the original file
output = read.csv("output.csv") #load in the scored data
dat$fifty = output$scored
write.csv(dat,
file = "max_huff processed.csv", row.names = F)
setwd("~/GitHub/lrd/Test Data/Rescored")
####setup####
dat = read.csv("max_buch processed.csv")
library(caret)
####sensitivity analysis####
#set up
data100 = factor(dat$one_hundred)
data95 = factor(dat$ninety_five)
data90 = factor(dat$ninety)
data85 = factor(dat$eightyfive)
data80 = factor(dat$eighty)
data75 = factor(dat$seventy_five)
data70 = factor(dat$seventy)
data65 = factor(dat$sixty_five)
data60 = factor(dat$sixty)
data55 = factor(dat$fifty_five)
data50 = factor(dat$fifty)
ref = factor(dat$manually_coded)
#start with 100
table(data100, ref)
sensitivity(data100, ref)
specificity(data100, ref)
#95
table(data95, ref)
sensitivity(data95, ref)
specificity(data95, ref)
#90
table(data90, ref)
sensitivity(data90, ref)
specificity(data90, ref)
#85
table(data85, ref)
sensitivity(data85, ref)
specificity(data85, ref)
#80
table(data80, ref)
sensitivity(data80, ref)
specificity(data80, ref)
#75
table(data75, ref)
sensitivity(data75, ref)
specificity(data75, ref)
#70
table(data70, ref)
sensitivity(data70, ref)
specificity(data70, ref)
#65
table(data65, ref)
sensitivity(data65, ref)
specificity(data65, ref)
#60
table(data60, ref)
sensitivity(data60, ref)
specificity(data60, ref)
#55
table(data55, ref)
sensitivity(data55, ref)
specificity(data55, ref)
#50
table(data50, ref)
sensitivity(data50, ref)
specificity(data50, ref)
sensitivity(data100, ref)
specificity(data100, ref)
sensitivity(data95, ref)
specificity(data95, ref)
sensitivity(data90, ref)
specificity(data90, ref)
sensitivity(data85, ref)
specificity(data85, ref)
sensitivity(data80, ref)
specificity(data80, ref)
sensitivity(data75, ref)
specificity(data75, ref)
sensitivity(data70, ref)
specificity(data70, ref)
sensitivity(data65, ref)
specificity(data65, ref)
sensitivity(data60, ref)
specificity(data60, ref)
sensitivity(data55, ref)
specificity(data55, ref)
####setup####
dat = read.csv("max_huff processed.csv")
library(caret)
####sensitivity analysis####
#set up
data100 = factor(dat$one_hundred)
data95 = factor(dat$ninety_five)
data90 = factor(dat$ninety)
data85 = factor(dat$eightyfive)
data80 = factor(dat$eighty)
data75 = factor(dat$seventy_five)
data70 = factor(dat$seventy)
data65 = factor(dat$sixty_five)
data60 = factor(dat$sixty)
data55 = factor(dat$fifty_five)
data50 = factor(dat$fifty)
ref = factor(dat$manually_coded)
#start with 100
table(data100, ref)
sensitivity(data100, ref)
specificity(data100, ref)
#95
table(data95, ref)
sensitivity(data95, ref)
specificity(data95, ref)
#90
table(data90, ref)
sensitivity(data90, ref)
specificity(data90, ref)
#85
table(data85, ref)
sensitivity(data85, ref)
specificity(data85, ref)
#80
table(data80, ref)
sensitivity(data80, ref)
specificity(data80, ref)
#75
table(data75, ref)
sensitivity(data75, ref)
specificity(data75, ref)
#70
table(data70, ref)
sensitivity(data70, ref)
specificity(data70, ref)
#65
table(data65, ref)
sensitivity(data65, ref)
specificity(data65, ref)
#60
table(data60, ref)
sensitivity(data60, ref)
specificity(data60, ref)
#55
table(data55, ref)
sensitivity(data55, ref)
specificity(data55, ref)
#50
table(data50, ref)
sensitivity(data50, ref)
specificity(data50, ref)
sensitivity(data100, ref)
specificity(data100, ref)
sensitivity(data95, ref)
specificity(data95, ref)
sensitivity(data90, ref)
specificity(data90, ref)
sensitivity(data85, ref)
specificity(data85, ref)
sensitivity(data80, ref)
specificity(data80, ref)
sensitivity(data75, ref)
specificity(data75, ref)
sensitivity(data70, ref)
specificity(data70, ref)
sensitivity(data65, ref)
specificity(data65, ref)
sensitivity(data60, ref)
specificity(data60, ref)
sensitivity(data55, ref)
specificity(data55, ref)
dat1 = read.csv("max_buch processed.csv")
dat2 = read.csv("max_huff processed.csv")
length(unique(dat1$Sub.ID))
length(unique(dat2$Sub.ID))
dat2 = na.omit(dat2)
#item level
corr.test(dat1$manually_coded, dat1[ , 8:17]) #Maxwell Buchananan
corr.test(dat2$manually_coded, dat2[ , 8:17]) #Maxwell Huff
#participant level
library(psych)
c1 = cohen.kappa(dat1[ , c(6, 8:18)]) #M & B
c1
print(c1, all = T)
cohen.kappa(dat2[ , c(6, 8:18)]) #M & H
cohen.k
####Descriptives, ANOVAS, t-tests####
library(ez)
library(reshape)
#set up for ANOVA
long.dat1 = melt(dat1[ , c(1, 6, 8:18)],
id = c("Sub.ID"))
colnames(long.dat1)[2] = "score_type"
colnames(long.dat1)[3] = "score"
long.dat2 = melt(dat2[ , c(2, 6, 8:18)],
id = c("Sub.ID"))
colnames(long.dat2)[2] = "score_type"
colnames(long.dat2)[3] = "score"
long.dat1$score = long.dat1$score * 100
long.dat2$score = long.dat2$score * 100
#convert to subject level data
final.1 = cast(long.dat1, Sub.ID ~ score_type, mean)
final.2 = cast(long.dat2, Sub.ID ~ score_type, mean)
anova.1 = melt(final.1,
id = "Sub.ID")
colnames(anova.1)[2] = "score"
colnames(anova.1)[3] = "score_type"
anova.2 = melt(final.2,
id = "Sub.ID")
colnames(anova.2)[2] = "score"
colnames(anova.2)[3] = "score_type"
#Anova time!
model1 = ezANOVA(anova.1,
dv = score,
wid = Sub.ID,
between = score_type,
detailed = T,
type = 3) #Non-SIG
model1
anovaLength = length(model1$ANOVA)
model1$ANOVA$MSE = model1$ANOVA$SSd/model1$ANOVA$DFd
model1$ANOVA$MSE
##get t values for comparison
library(reshape)
tapply(anova.1$score,
anova.1$score_type, mean)
#get 95%CI
tap1 = tapply(anova.1$score,
anova.1$score_type, sd)
se1 = tap1 / sqrt(length(unique(anova.1$Sub.ID)))
se1 * 1.96
mb_posthoc = cast(long.dat1, Sub.ID ~ score_type, mean)
t.test(mb_posthoc$fifty, mb_posthoc$one_hundred, paired = F, p.adjust.methods = "bonferroni") #sig
##Now do the maxwell and huff data
model2 = ezANOVA(anova.2,
dv = score,
wid = Sub.ID,
between = score_type,
detailed = T,
type = 3)
model2
anovaLength2 = length(model2$ANOVA)
model2$ANOVA$MSE = model2$ANOVA$SSd/model2$ANOVA$DFd
model2$ANOVA$MSE
##now do the other dataset
tapply(anova.2$score,
anova.2$score_type, mean)
tap2 = tapply(anova.2$score,
anova.2$score_type, sd)
se2 = tap2 / sqrt(length(unique(anova.2$Sub.ID)))
se2 * 1.96
#get t-values
mh_posthoc = cast(long.dat2, Sub.ID ~ score_type, mean)
temp = t.test(mh_posthoc$fifty, mh_posthoc$sixty, paired = F, p.adjust.methods = "bonferroni")
temp
p4 = round(temp$p.value, 3)
t4 = temp$statistic
SEM4 = (temp$conf.int[2] - temp$conf.int[1]) / 3.92
SEM4
print(c1, all = T)
cohen.k
c1
c1 = cohen.kappa(dat1[ , c(6, 8:18)]) #M & B
print(c1, all = T)
####Descriptives, ANOVAS, t-tests####
library(ez)
c1
cohen.kappa(dat2[ , c(6, 8:18)]) #M & H
View(mb_posthoc)
tap1
tapply(anova.1$score,
anova.1$score_type, mean)
se1 * 1.96
se1
#get 95%CI
tap1 = tapply(anova.1$score,
anova.1$score_type, sd)
tap1
127/2
##now do the other dataset
tapply(anova.2$score,
anova.2$score_type, mean)
##now do the other dataset
tapply(anova.2$score,
anova.2$score_type, mean)
tap2 = tapply(anova.2$score,
anova.2$score_type, sd)
tap2
se2 = tap2 / sqrt(length(unique(anova.2$Sub.ID)))
se2 * 1.96
##Now do the maxwell and huff data
model2 = ezANOVA(anova.2,
dv = score,
wid = Sub.ID,
between = score_type,
detailed = T,
type = 3)
model2
##now do the other dataset
tapply(anova.2$score,
anova.2$score_type, mean)
#get t-values
mh_posthoc = cast(long.dat2, Sub.ID ~ score_type, mean)
temp = t.test(mh_posthoc$fifty, mh_posthoc$sixty, paired = F, p.adjust.methods = "bonferroni")
temp
p4 = round(temp$p.value, 3)
t4 = temp$statistic
temp
##now do the other dataset
tapply(anova.2$score,
anova.2$score_type, mean)
model2
model2$ANOVA$MSE = model2$ANOVA$SSd/model2$ANOVA$DFd
model2$ANOVA$MSE
temp
temp = t.test(mh_posthoc$fifty, mh_posthoc$fifty_five, paired = F, p.adjust.methods = "bonferroni")
temp
temp = t.test(mh_posthoc$fifty, mh_posthoc$sixty, paired = F, p.adjust.methods = "bonferroni")
temp
temp = t.test(mh_posthoc$fifty, mh_posthoc$manually_coded, paired = F, p.adjust.methods = "bonferroni")
temp
p4 = round(temp$p.value, 3)
t4 = temp$statistic
SEM4 = (temp$conf.int[2] - temp$conf.int[1]) / 3.92
SEM4
##now do the other dataset
tapply(anova.2$score,
anova.2$score_type, mean)
##now do the other dataset
tapply(anova.2$score,
anova.2$score_type, mean)
tap2 = tapply(anova.2$score,
anova.2$score_type, sd)
tap2
temp = t.test(mh_posthoc$one_hundred, mh_posthoc$manually_coded, paired = F, p.adjust.methods = "bonferroni")
temp
p4 = round(temp$p.value, 3)
t4 = temp$statistic
temp
temp = t.test(mh_posthoc$fifty, mh_posthoc$sixty_five, paired = F, p.adjust.methods = "bonferroni")
temp
##now do the other dataset
tapply(anova.2$score,
anova.2$score_type, mean)
tap2
#Anova time!
model1 = ezANOVA(anova.1,
dv = score,
wid = Sub.ID,
between = score_type,
detailed = T,
type = 3) #Non-SIG
model1
anovaLength = length(model1$ANOVA)
model1$ANOVA$MSE = model1$ANOVA$SSd/model1$ANOVA$DFd
model1$ANOVA$MSE
View(anova.2)
View(long.dat1)
View(dat1)
prop.correct(dat1$one_hundred, id = dat1$Sub.ID)
prop.correct(dat1$one_hundred, id = dat1$Sub.ID, group.by = dat1$Cue)
runApp('~/GitHub/lrd/lrdSHINY')
