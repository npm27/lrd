int.dat$diff = int.dat$jol_bin - int.dat$mean_recall
library(car)
model9 = aov(int.dat$diff ~ (int.dat$jol_bin*int.dat$direction))
summary(model9)
library(lsr)
etas1 = etaSquared(model9)
bar3 = ggplot(long.dat, aes(Direction, Score, fill = Task))
bar3 = bar3 +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "Black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = 0.2,
color = "black") +
scale_fill_manual("Task",
values = c("Jol" = "white",
"Recall" = "dimgrey")) +
cleanup +
xlab("Direction") +
ylab("Mean Task Performance")
ylim(0,100)
#labs(title="All Blocks")
bar3
dat = read.csv("ex2 final output.csv")
summary(dat)
library(ggplot2)
library(reshape)
##put recall on correct scale
dat$Scored_Response = (dat$Scored_Response * 100)
##remove out of range scores
dat$Jol_Response[dat$Jol_Response > 100] = NA
##get sample size
summary(dat$Subject) #n = 34
summary(dat)
##remove missing
nomiss3 = na.omit(dat)
colnames(nomiss3)[6] = "Jol"
colnames(nomiss3)[9] = "Recall"
####make the graph####
##melt the data
long.dat = melt(nomiss3, id = c("Subject", "Block",
"ListNum", "Direction", "ExperimentName", "cue_target",
"recall_response", "cue_prompt"))
summary(long.dat)
colnames(long.dat)[9] = "Task"
colnames(long.dat)[10] = "Score"
bar3 = ggplot(long.dat, aes(Direction, Score, fill = Task))
bar3 = bar3 +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "Black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = 0.2,
color = "black") +
scale_fill_manual("Task",
values = c("Jol" = "white",
"Recall" = "dimgrey")) +
cleanup +
xlab("Direction") +
ylab("Mean Task Performance")
ylim(0,100)
#labs(title="All Blocks")
bar3
bar3 = ggplot(long.dat, aes(Direction, Score, fill = Task))
bar3 = bar3 +
stat_summary(fun.y = mean,
geom = "bar",
position = "dodge",
color = "Black") +
stat_summary(fun.data = mean_cl_normal,
geom = "errorbar",
position = position_dodge(width = 0.90),
width = 0.2,
color = "black") +
scale_fill_manual("Task",
values = c("Jol" = "white",
"Recall" = "dimgrey")) +
cleanup +
xlab("Direction") +
ylab("Mean Task Performance") +
ylim(0,100)
#labs(title="All Blocks")
bar3
knitr::include_graphics("plot1.png")
citr:::insert_citation()
12.11*40
484.4*2
968.8*.15
968.8-145.32
823-110-50
85-22)
85-22
n = 100
dbinom(100)
dbinom(50, 100, 50)
dbinom(50, 100, 2)
dbinom(50, 100, .5)
dbinom(50, 100, .5)
round(data.frame(0:100, probs), digits = 5)
probs = dbinom(50, 100, .5)
round(data.frame(0:100, probs), digits = 5)
plot(0:100, probs, type="h", xlim=c(0,100), ylim=c(0,.1))
probs = dbinom(0:100, 100, .5) ##get the probability
round(data.frame(0:100, probs), digits = 5)
plot(0:100, probs, type="h", xlim=c(0,100), ylim=c(0,.1))
points(0:100, probs, pch=16, cex=.5)
curve(dnorm(x, mean=50, sd=5), from=0, to=100, xlim = c(0, 100), ylim = c(0, 0.5), xlab = "x", add=T, col="blue")
sum(dbinom(45:55, size=100, prob=1/2))
sum(dbinom(50, size=100, prob=1/2))
length(probs)
probs = as.data.frame(probs)
subset(probs, Mod(probs$probs) == 0)
View(probs)
head = 1
tail = 0
prob = e
e
e = exp(1)
sample(c("Heads", "Tails"), n, rep = T)
Flip1Coin = function(n) sample(c("Heads", "Tails"), n, rep = T)
Flip1Coin(n)
sample(c("Heads", "Tails"), n, rep = T)
sample(c("Heads", "Tails"), n, rep = 100)
Flip1Coin = function(n) sample(c("Heads", "Tails"), n, rep = 100)
Flip1Coin(n)
sample(c("Heads", "Tails"), n, rep = 100)
Flip1Coin = sample(c("Heads", "Tails"), n, rep = 100)
pbinom(50, 100, .5)
dbinom(50, 100, .5)
dnorm(50)
dnorm(50/sqrt(24))
pbinom(50, size = 100, .5)
pbinom(48, size = 100, .5)
pbinom(52, size = 100, .5)
pbinom(71, size = 100, .5)
m = 100 * .5
sd1 = sqrt(100 * .5 *.5)
1 - dnorm(50, mean = m, sd = sd1)
1 - pnorm(50, mean = m, sd = sd1)
x = c(0:100)
length(x)
mod(x, 2)
Mod(x)
Mod(x, 2)
.5*10
.75*7.5
.6*7.5
.7*7.5
.65*7.5
.68*7.5
.625*7.5
.667*7.5
.667*5
.667*4
.667*4.25
.667*8
.6 *8
.62 *8
.63 *8
.625 *8
.625 * 4
25*60
1500/5
install.packages("installr")
installr::installr()
library(ez)
library(reshape)
setwd("~/GitHub/lrd/Sentences")
####Lrd Sentence processing####
#Load some libraries
library(dplyr) #Can't remember I ended up using this package...
library(koRpus)
library(rapportools)
library(quanteda)
source("percent_match_s.r")
##Okay, I need this to work at the dataframe level...
#But, let's focus on something a smaller for now....
#s1 = "This is a sentence."
#s2 = "This is also a sentencee."
##First thing I should do is figure out how to split each into their respective tokens
#S1 is going to be the cue (so the value stored in the key)
#S2 will represent the participant's response.
##Basically, step 1 break things into tokens
##Step 2: Figure out the percent match
##2a: Number of words
##2b: Spelling errors
##Step 3: Figure out words omitted and return them
##Step 4: Figure out extra words and return them
#Read in some data
dat = read.csv("sentences example.csv")
##For the function, inputs will need to be:
#participant responses (x)
#the answer key (y)
#the subject number (id)
#and a percent match cutoff for the spelling stuff (z)
#Will need to figure out how to suppress those treetagger warnings
s1 = dat$Sentence
s2 = dat$Response
id = dat$Sub.ID
input = data.frame(id, s1, s2)
#Create blank containers for the output
s_match = c()
shared3 = c()
omitted3 = c()
extras3 = c()
corrected = c()
##Loop through subject
suppressWarnings(for (i in unique(input$id)) {
temp = subset(input,
input$id == i)
#print(temp)
#Loop through what subject was supposed to type
for (j in temp$s1){
temp2 = subset(temp,
temp$s1 == j)
#print(temp2)
#Step 1: Get tokens
s1.tokens = tokens(temp2$s1, lang = "en")
s2.tokens = tokens(temp2$s2, lang = "en")
#print(s1.tokens)
#print(s2.tokens)
##Step 2a: Figure out number of words in each
shared = intersect(s1.tokens, s2.tokens) #This works... assuming that participants always spell things correctly
omitted = setdiff(s1.tokens, s2.tokens) #Everything omitted
extras = setdiff(s2.tokens, s1.tokens)
##step 2b: Spelling
#Can use omitted as an answer key. Basically just check each extra against each omitted and find the thing with the highest match.
#Can probably solve my problems with a loop...
#Make some empty things for storage
temp_i2 = c()
temp_j2 = c()
temp_b2 = c()
#No extra words or omitted words -- sentences match perfectly
if (is.empty(omitted) == TRUE & is.empty(extras) == TRUE){
temp_i2 = c(temp_i2, " ")
temp_j2 = c(temp_j2, " ")
temp_b2 = c(temp_b2, 1)
#print(temp_i)
output = cbind(temp_i2, temp_j2, temp_b2)
output = data.frame(output)
colnames(output)[1:3] = c("Key", "Response", "Percent_Match")
#print(output)
#No omitted words but extra words are included
} else if (is.empty(omitted) == TRUE & is.empty(extras) == FALSE){
temp_i3 = c()
temp_j3 = c()
temp_b3 = c()
temp_i3 = c(temp_i3, " ")
for (q in extras){
temp_j3 = c(temp_j3, q)
}
b = length(s1.tokens) / length(s2.tokens)
temp_b3 = c(temp_b3, b)
output = cbind(temp_i3, temp_j3, temp_b3)
output = data.frame(output)
colnames(output)[1:3] = c("Key", "Response", "Percent_Match")
##No extra words but some words are omitted
} else if (is.empty(omitted) == FALSE & is.empty(extras) == TRUE) {
temp_i4 = c()
temp_j4 = c()
temp_b4 = c()
temp_j4 = c(temp_j4, " ")
for (xx in omitted){
temp_i4 = c(temp_i4, xx)
}
b = length(s2.tokens) / length(s1.tokens)
temp_b4 = c(temp_b4, b)
output = cbind(temp_i4, temp_j4, temp_b4)
output = data.frame(output)
colnames(output)[1:3] = c("Key", "Response", "Percent_Match")
#Other situations where sentences don't match perfectly
} else {
temp_i = c()
temp_j = c()
temp_b = c()
for (k in omitted){
for (p in extras){
b = percent_match.s(k, p)
#print(k)
#print(p)
#print(b)
temp_b = c(temp_b, b)
temp_i = c(temp_i, k)
temp_j = c(temp_j, p)
}
#temp_b = data.frame(temp_b)
#temp_i = data.frame(temp_i)
#temp_j = data.frame(temp_j)
#colnames(temp_b)[1] = "Percent_Match"
#colnames(temp_i)[1] = "Key"
#colnames(temp_j)[1] = "Response"
output = cbind(temp_i, temp_j, temp_b)
#print(output)
}
output = data.frame(output)
colnames(output)[1:3] = c("Key", "Response", "Percent_Match")
##Now fix the spelling errors
##output8 is all the spelling errors I want fixed
output8 = subset(output,
output$Percent_Match >= .75 & output$Percent_Match < 1) #Using a 75% percent match here, but ideally I would want this to be user selected
##Output 9 is all of the extra words participants typed
output9 = subset(output,
output$Percent_Match <= .75)
##Output 10 is for when sentences match perfectly
output10 = subset(output,
output$Percent_Match == 1)
##If output 8 has stuff in it (basically if tokens in the participant response need changing):
if (is.empty(output8$Key) == FALSE){
#Fix the spelling errors
temp2$s2 = gsub(output8[[2]], output8[[1]], temp2$s2)
#Now get the percentage of word match after correcting for spelling
s2.tokens = tokens(temp2$s2, lang = "en")
##If there's nothing that's a close enough match
} else if (is.empty(output8$key) == TRUE){
temp2$s2 = temp2$s2
s2.tokens = s2.tokens
}
}
###Start slapping together the output file.
##Basically what I'd like is input, a sentence match column, an extra word column, and an omitted word column
shared2 = intersect(s1.tokens, s2.tokens)
s_match2 = length(shared2) / max(c(length(s1.tokens), length(s2.tokens)))
omitted2 = toString(setdiff(s1.tokens, s2.tokens)) #Final omitted
extras2 = toString(setdiff(s2.tokens, s1.tokens))
print(temp2$id)
shared3 = c(shared3, shared2)
s_match = c(s_match, s_match2)
omitted3 = c(omitted3, omitted2)
extras3 = c(extras3, extras2)
#Also think it would be good to return the corrected responses
}
})
final = data.frame(id, s1, s2, s_match, omitted3, extras3)
colnames(final)[1:6] = c("ID", "Key", "Response", "Percent_Match", "Omitted_Items", "Extra_Items")
View(temp2)
####Lrd Sentence processing####
#Load some libraries
library(dplyr) #Can't remember I ended up using this package...
library(koRpus)
library(rapportools)
library(quanteda)
source("percent_match_s.r")
##Okay, I need this to work at the dataframe level...
#But, let's focus on something a smaller for now....
#s1 = "This is a sentence."
#s2 = "This is also a sentencee."
##First thing I should do is figure out how to split each into their respective tokens
#S1 is going to be the cue (so the value stored in the key)
#S2 will represent the participant's response.
##Basically, step 1 break things into tokens
##Step 2: Figure out the percent match
##2a: Number of words
##2b: Spelling errors
##Step 3: Figure out words omitted and return them
##Step 4: Figure out extra words and return them
#Read in some data
dat = read.csv("sentences example.csv")
##For the function, inputs will need to be:
#participant responses (x)
#the answer key (y)
#the subject number (id)
#and a percent match cutoff for the spelling stuff (z)
#Will need to figure out how to suppress those treetagger warnings
s1 = dat$Sentence
s2 = dat$Response
id = dat$Sub.ID
input = data.frame(id, s1, s2)
#Create blank containers for the output
s_match = c()
shared3 = c()
omitted3 = c()
extras3 = c()
corrected = c()
##Loop through subject
suppressWarnings(for (i in unique(input$id)) {
temp = subset(input,
input$id == i)
#print(temp)
#Loop through what subject was supposed to type
for (j in temp$s1){
temp2 = subset(temp,
temp$s1 == j)
#print(temp2)
#Step 1: Get tokens
s1.tokens = tokens(temp2$s1, lang = "en")
s2.tokens = tokens(temp2$s2, lang = "en")
#print(s1.tokens)
#print(s2.tokens)
##Step 2a: Figure out number of words in each
shared = intersect(s1.tokens, s2.tokens) #This works... assuming that participants always spell things correctly
omitted = setdiff(s1.tokens, s2.tokens) #Everything omitted
extras = setdiff(s2.tokens, s1.tokens)
##step 2b: Spelling
#Can use omitted as an answer key. Basically just check each extra against each omitted and find the thing with the highest match.
#Can probably solve my problems with a loop...
#Make some empty things for storage
temp_i2 = c()
temp_j2 = c()
temp_b2 = c()
#No extra words or omitted words -- sentences match perfectly
if (is.empty(omitted) == TRUE & is.empty(extras) == TRUE){
temp_i2 = c(temp_i2, " ")
temp_j2 = c(temp_j2, " ")
temp_b2 = c(temp_b2, 1)
#print(temp_i)
output = cbind(temp_i2, temp_j2, temp_b2)
output = data.frame(output)
colnames(output)[1:3] = c("Key", "Response", "Percent_Match")
#print(output)
#No omitted words but extra words are included
} else if (is.empty(omitted) == TRUE & is.empty(extras) == FALSE){
temp_i3 = c()
temp_j3 = c()
temp_b3 = c()
temp_i3 = c(temp_i3, " ")
for (q in extras){
temp_j3 = c(temp_j3, q)
}
b = length(s1.tokens) / length(s2.tokens)
temp_b3 = c(temp_b3, b)
output = cbind(temp_i3, temp_j3, temp_b3)
output = data.frame(output)
colnames(output)[1:3] = c("Key", "Response", "Percent_Match")
##No extra words but some words are omitted
} else if (is.empty(omitted) == FALSE & is.empty(extras) == TRUE) {
temp_i4 = c()
temp_j4 = c()
temp_b4 = c()
temp_j4 = c(temp_j4, " ")
for (xx in omitted){
temp_i4 = c(temp_i4, xx)
}
b = length(s2.tokens) / length(s1.tokens)
temp_b4 = c(temp_b4, b)
output = cbind(temp_i4, temp_j4, temp_b4)
output = data.frame(output)
colnames(output)[1:3] = c("Key", "Response", "Percent_Match")
#Other situations where sentences don't match perfectly
} else {
temp_i = c()
temp_j = c()
temp_b = c()
for (k in omitted){
for (p in extras){
b = percent_match.s(k, p)
#print(k)
#print(p)
#print(b)
temp_b = c(temp_b, b)
temp_i = c(temp_i, k)
temp_j = c(temp_j, p)
}
#temp_b = data.frame(temp_b)
#temp_i = data.frame(temp_i)
#temp_j = data.frame(temp_j)
#colnames(temp_b)[1] = "Percent_Match"
#colnames(temp_i)[1] = "Key"
#colnames(temp_j)[1] = "Response"
output = cbind(temp_i, temp_j, temp_b)
print(output)
}
output = data.frame(output)
colnames(output)[1:3] = c("Key", "Response", "Percent_Match")
##Now fix the spelling errors
##output8 is all the spelling errors I want fixed
output8 = subset(output,
output$Percent_Match >= .75 & output$Percent_Match < 1) #Using a 75% percent match here, but ideally I would want this to be user selected
##Output 9 is all of the extra words participants typed
output9 = subset(output,
output$Percent_Match <= .75)
##Output 10 is for when sentences match perfectly
output10 = subset(output,
output$Percent_Match == 1)
##If output 8 has stuff in it (basically if tokens in the participant response need changing):
if (is.empty(output8$Key) == FALSE){
#Fix the spelling errors
temp2$s2 = gsub(output8[[2]], output8[[1]], temp2$s2)
#Now get the percentage of word match after correcting for spelling
s2.tokens = tokens(temp2$s2, lang = "en")
##If there's nothing that's a close enough match
} else if (is.empty(output8$key) == TRUE){
temp2$s2 = temp2$s2
s2.tokens = s2.tokens
}
}
###Start slapping together the output file.
##Basically what I'd like is input, a sentence match column, an extra word column, and an omitted word column
shared2 = intersect(s1.tokens, s2.tokens)
s_match2 = length(shared2) / max(c(length(s1.tokens), length(s2.tokens)))
omitted2 = toString(setdiff(s1.tokens, s2.tokens)) #Final omitted
extras2 = toString(setdiff(s2.tokens, s1.tokens))
print(temp2$id)
shared3 = c(shared3, shared2)
s_match = c(s_match, s_match2)
omitted3 = c(omitted3, omitted2)
extras3 = c(extras3, extras2)
#Also think it would be good to return the corrected responses
}
})
final = data.frame(id, s1, s2, s_match, omitted3, extras3)
colnames(final)[1:6] = c("ID", "Key", "Response", "Percent_Match", "Omitted_Items", "Extra_Items")
setdiff(s2.tokens, s1.tokens)
s2.tokens
setdiff(s2.tokens, s1.tokens)
intersect(s1.tokens, s2.tokens)
intersect(s1.tokens, s2.tokens)
