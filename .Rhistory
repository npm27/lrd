} #only if extras is a thing
j
corrected <- c(corrected, extras[j])
corrected
#cheap tokenization
answer.tokens <- unlist(strsplit(DF$Answer[i], split = token.split))
response.tokens <- unlist(strsplit(DF$Responses[i], split = token.split))
#figure out number of words in each
shared <- c()
omitted <- c()
extras <- c()
corrected <- c()
shared <- intersect(answer.tokens, response.tokens)
omitted <- setdiff(answer.tokens, response.tokens)
extras <- setdiff(response.tokens, answer.tokens)
omitted_final <- omitted
extras_final <- extras
#figure out if extra words are actually omitted words
if (length(extras) > 0){
for (j in 1:length(extras)){
lev_score <- adist(extras[j], omitted)
names(lev_score) <- omitted
#Find the minimum value for best match
#Figure out if the min score is within the cut off
if(min(lev_score) <= cutoff) {
corrected <- c(corrected, extras[j])
extras_final <- extras_final[!grepl(extras[j], extras_final)]
omitted_final <- omitted_final[!grepl(attr(which.min(lev_score), "names"), omitted_final)]
} #figure out min lev
} #going through extra words
} #only if extras is a thing
extras
omitted
extras_final
omitted_Final
omitted_final
#dear lord a loop is the best idea
for (i in 1:nrow(DF)){
#cheap tokenization
answer.tokens <- unlist(strsplit(DF$Answer[i], split = token.split))
response.tokens <- unlist(strsplit(DF$Responses[i], split = token.split))
#figure out number of words in each
shared <- c()
omitted <- c()
extras <- c()
corrected <- c()
shared <- intersect(answer.tokens, response.tokens)
omitted <- setdiff(answer.tokens, response.tokens)
extras <- setdiff(response.tokens, answer.tokens)
omitted_final <- omitted
extras_final <- extras
#figure out if extra words are actually omitted words
if (length(extras) > 0){
for (j in 1:length(extras)){
lev_score <- adist(extras[j], omitted)
names(lev_score) <- omitted
#Find the minimum value for best match
#Figure out if the min score is within the cut off
if(min(lev_score) <= cutoff) {
corrected <- c(corrected, extras[j])
extras_final <- extras_final[!grepl(extras[j], extras_final)]
omitted_final <- omitted_final[!grepl(attr(which.min(lev_score), "names"), omitted_final)]
} #figure out min lev
} #going through extra words
} #only if extras is a thing
if(length(shared) > 0){ DF$Shared.Items[i] <- paste(shared, collapse = " ") } else {
DF$Shared.Items[i] <- NA
}
if(length(omitted) > 0){ DF$Omitted.Items[i] <- paste(omitted_final, collapse = " ") } else {
DF$Omitted.Items <- NA
}
if(length(corrected) > 0){ DF$Corrected.Items[i] <- paste(corrected, collapse = " ") } else {
DF$Corrected.Items <- NA
}
if(length(extras) > 0){ DF$Extra.Items[i] <- paste(extras_final, collapse = " ") } else {
DF$Extra.Items <- NA
}
DF$Proportion.Match[i] <- (length(shared) + length(corrected)) / length(answer.tokens)
}
i
answer.tokens
response.tokens
shared
j
omittted
extras
lev_score
min(lev_score) <= cutoff
lev_score <- adist(extras[j], omitted)
names(lev_score) <- omitted
lev_score
#Find the minimum value for best match
#Figure out if the min score is within the cut off
if(min(lev_score) <= cutoff) {
corrected <- c(corrected, extras[j])
extras_final <- extras_final[!grepl(extras[j], extras_final)]
omitted_final <- omitted_final[!grepl(attr(which.min(lev_score), "names"), omitted_final)]
} #figure out min lev
j = 2
lev_score <- adist(extras[j], omitted)
names(lev_score) <- omitted
#Find the minimum value for best match
#Figure out if the min score is within the cut off
if(min(lev_score) <= cutoff) {
corrected <- c(corrected, extras[j])
extras_final <- extras_final[!grepl(extras[j], extras_final)]
omitted_final <- omitted_final[!grepl(attr(which.min(lev_score), "names"), omitted_final)]
} #figure out min lev
lev_score
i
#cheap tokenization
answer.tokens <- unlist(strsplit(DF$Answer[i], split = token.split))
response.tokens <- unlist(strsplit(DF$Responses[i], split = token.split))
#figure out number of words in each
shared <- c()
omitted <- c()
extras <- c()
corrected <- c()
shared <- intersect(answer.tokens, response.tokens)
omitted <- setdiff(answer.tokens, response.tokens)
extras <- setdiff(response.tokens, answer.tokens)
omitted_final <- omitted
extras_final <- extras
shared
omitted
extras
i = 1
i = 30
j = 1
lev_score <- adist(extras[j], omitted)
names(lev_score) <- omitted
#Find the minimum value for best match
#Figure out if the min score is within the cut off
if(min(lev_score) <= cutoff) {
corrected <- c(corrected, extras[j])
extras_final <- extras_final[!grepl(extras[j], extras_final)]
omitted_final <- omitted_final[!grepl(attr(which.min(lev_score), "names"), omitted_final)]
} #figure out min lev
j = 2
lev_score <- adist(extras[j], omitted)
names(lev_score) <- omitted
#Find the minimum value for best match
#Figure out if the min score is within the cut off
if(min(lev_score) <= cutoff) {
corrected <- c(corrected, extras[j])
extras_final <- extras_final[!grepl(extras[j], extras_final)]
omitted_final <- omitted_final[!grepl(attr(which.min(lev_score), "names"), omitted_final)]
} #figure out min lev
omitted_final
extra_final
extras_final
j = 3
lev_score <- adist(extras[j], omitted)
names(lev_score) <- omitted
#Find the minimum value for best match
#Figure out if the min score is within the cut off
if(min(lev_score) <= cutoff) {
corrected <- c(corrected, extras[j])
extras_final <- extras_final[!grepl(extras[j], extras_final)]
omitted_final <- omitted_final[!grepl(attr(which.min(lev_score), "names"), omitted_final)]
} #figure out min lev
lev_score
if(length(shared) > 0){ DF$Shared.Items[i] <- paste(shared, collapse = " ") } else {
DF$Shared.Items[i] <- NA
}
if(length(omitted_final) > 0){ DF$Omitted.Items[i] <- paste(omitted_final, collapse = " ") } else {
DF$Omitted.Items <- NA
}
if(length(corrected) > 0){ DF$Corrected.Items[i] <- paste(corrected, collapse = " ") } else {
DF$Corrected.Items <- NA
}
if(length(extras_final) > 0){ DF$Extra.Items[i] <- paste(extras_final, collapse = " ") } else {
DF$Extra.Items <- NA
}
DF$Proportion.Match[i] <- (length(shared) + length(corrected)) / length(answer.tokens)
#dear lord a loop is the best idea
for (i in 1:nrow(DF)){
#cheap tokenization
answer.tokens <- unlist(strsplit(DF$Answer[i], split = token.split))
response.tokens <- unlist(strsplit(DF$Responses[i], split = token.split))
#figure out number of words in each
shared <- c()
omitted <- c()
extras <- c()
corrected <- c()
shared <- intersect(answer.tokens, response.tokens)
omitted <- setdiff(answer.tokens, response.tokens)
extras <- setdiff(response.tokens, answer.tokens)
omitted_final <- omitted
extras_final <- extras
#figure out if extra words are actually omitted words
if (length(extras) > 0){
for (j in 1:length(extras)){
lev_score <- adist(extras[j], omitted)
names(lev_score) <- omitted
#Find the minimum value for best match
#Figure out if the min score is within the cut off
if(min(lev_score) <= cutoff) {
corrected <- c(corrected, extras[j])
extras_final <- extras_final[!grepl(extras[j], extras_final)]
omitted_final <- omitted_final[!grepl(attr(which.min(lev_score), "names"), omitted_final)]
} #figure out min lev
} #going through extra words
} #only if extras is a thing
if(length(shared) > 0){ DF$Shared.Items[i] <- paste(shared, collapse = " ") } else {
DF$Shared.Items[i] <- NA
}
if(length(omitted_final) > 0){ DF$Omitted.Items[i] <- paste(omitted_final, collapse = " ") } else {
DF$Omitted.Items <- NA
}
if(length(corrected) > 0){ DF$Corrected.Items[i] <- paste(corrected, collapse = " ") } else {
DF$Corrected.Items <- NA
}
if(length(extras_final) > 0){ DF$Extra.Items[i] <- paste(extras_final, collapse = " ") } else {
DF$Extra.Items <- NA
}
DF$Proportion.Match[i] <- (length(shared) + length(corrected)) / length(answer.tokens)
}
lev_score
min(lev_score)
#dear lord a loop is the best idea
for (i in 1:nrow(DF)){
#cheap tokenization
answer.tokens <- unlist(strsplit(DF$Answer[i], split = token.split))
response.tokens <- unlist(strsplit(DF$Responses[i], split = token.split))
#figure out number of words in each
shared <- c()
omitted <- c()
extras <- c()
corrected <- c()
shared <- intersect(answer.tokens, response.tokens)
omitted <- setdiff(answer.tokens, response.tokens)
extras <- setdiff(response.tokens, answer.tokens)
omitted_final <- omitted
extras_final <- extras
#figure out if extra words are actually omitted words
#need both to check this
if (length(extras) > 0 & length(omitted) > 0){
for (j in 1:length(extras)){
lev_score <- adist(extras[j], omitted)
names(lev_score) <- omitted
#Find the minimum value for best match
#Figure out if the min score is within the cut off
if(min(lev_score) <= cutoff) {
corrected <- c(corrected, extras[j])
extras_final <- extras_final[!grepl(extras[j], extras_final)]
omitted_final <- omitted_final[!grepl(attr(which.min(lev_score), "names"), omitted_final)]
} #figure out min lev
} #going through extra words
} #only if extras is a thing
if(length(shared) > 0){ DF$Shared.Items[i] <- paste(shared, collapse = " ") } else {
DF$Shared.Items[i] <- NA
}
if(length(omitted_final) > 0){ DF$Omitted.Items[i] <- paste(omitted_final, collapse = " ") } else {
DF$Omitted.Items <- NA
}
if(length(corrected) > 0){ DF$Corrected.Items[i] <- paste(corrected, collapse = " ") } else {
DF$Corrected.Items <- NA
}
if(length(extras_final) > 0){ DF$Extra.Items[i] <- paste(extras_final, collapse = " ") } else {
DF$Extra.Items <- NA
}
DF$Proportion.Match[i] <- (length(shared) + length(corrected)) / length(answer.tokens)
}
View(DF)
i = 23
#cheap tokenization
answer.tokens <- unlist(strsplit(DF$Answer[i], split = token.split))
response.tokens <- unlist(strsplit(DF$Responses[i], split = token.split))
#figure out number of words in each
shared <- c()
omitted <- c()
extras <- c()
corrected <- c()
shared <- intersect(answer.tokens, response.tokens)
omitted <- setdiff(answer.tokens, response.tokens)
extras <- setdiff(response.tokens, answer.tokens)
omitted_final <- omitted
extras_final <- extras
answer.tokens
response.tokens
omitted
extra
extras
j = 1
lev_score <- adist(extras[j], omitted)
names(lev_score) <- omitted
#Find the minimum value for best match
#Figure out if the min score is within the cut off
if(min(lev_score) <= cutoff) {
corrected <- c(corrected, extras[j])
extras_final <- extras_final[!grepl(extras[j], extras_final)]
omitted_final <- omitted_final[!grepl(attr(which.min(lev_score), "names"), omitted_final)]
} #figure out min lev
j = 2
#figure out if extra words are actually omitted words
#need both to check this
if (length(extras) > 0 & length(omitted) > 0){
for (j in 1:length(extras)){
lev_score <- adist(extras[j], omitted)
names(lev_score) <- omitted
#Find the minimum value for best match
#Figure out if the min score is within the cut off
if(min(lev_score) <= cutoff) {
corrected <- c(corrected, extras[j])
extras_final <- extras_final[!grepl(extras[j], extras_final)]
omitted_final <- omitted_final[!grepl(attr(which.min(lev_score), "names"), omitted_final)]
} #figure out min lev
} #going through extra words
} #only if extras is a thing
shared
#cheap tokenization
answer.tokens <- unlist(strsplit(DF$Answer[i], split = token.split))
response.tokens <- unlist(strsplit(DF$Responses[i], split = token.split))
#figure out number of words in each
shared <- c()
omitted <- c()
extras <- c()
corrected <- c()
shared <- intersect(answer.tokens, response.tokens)
omitted <- setdiff(answer.tokens, response.tokens)
extras <- setdiff(response.tokens, answer.tokens)
omitted_final <- omitted
extras_final <- extras
#figure out if extra words are actually omitted words
#need both to check this
if (length(extras) > 0 & length(omitted) > 0){
for (j in 1:length(extras)){
lev_score <- adist(extras[j], omitted)
names(lev_score) <- omitted
#Find the minimum value for best match
#Figure out if the min score is within the cut off
if(min(lev_score) <= cutoff) {
corrected <- c(corrected, extras[j])
extras_final <- extras_final[!grepl(extras[j], extras_final)]
omitted_final <- omitted_final[!grepl(attr(which.min(lev_score), "names"), omitted_final)]
} #figure out min lev
} #going through extra words
} #only if extras is a thing
shared
omitted
extras
corrected
omitted_final
extras_final
length(shared) > 0
length(omitted_final) > 0
paste(omitted_final, collapse = " ")
length(corrected) > 0
length(extras_final) > 0
#dear lord a loop is the best idea
for (i in 1:nrow(DF)){
#cheap tokenization
answer.tokens <- unlist(strsplit(DF$Answer[i], split = token.split))
response.tokens <- unlist(strsplit(DF$Responses[i], split = token.split))
#figure out number of words in each
shared <- c()
omitted <- c()
extras <- c()
corrected <- c()
shared <- intersect(answer.tokens, response.tokens)
omitted <- setdiff(answer.tokens, response.tokens)
extras <- setdiff(response.tokens, answer.tokens)
omitted_final <- omitted
extras_final <- extras
#figure out if extra words are actually omitted words
#need both to check this
if (length(extras) > 0 & length(omitted) > 0){
for (j in 1:length(extras)){
lev_score <- adist(extras[j], omitted)
names(lev_score) <- omitted
#Find the minimum value for best match
#Figure out if the min score is within the cut off
if(min(lev_score) <= cutoff) {
corrected <- c(corrected, extras[j])
extras_final <- extras_final[!grepl(extras[j], extras_final)]
omitted_final <- omitted_final[!grepl(attr(which.min(lev_score), "names"), omitted_final)]
} #figure out min lev
} #going through extra words
} #only if extras is a thing
if(length(shared) > 0){ DF$Shared.Items[i] <- paste(shared, collapse = " ") } else {
DF$Shared.Items[i] <- NA
}
if(length(omitted_final) > 0){ DF$Omitted.Items[i] <- paste(omitted_final, collapse = " ") } else {
DF$Omitted.Items[i] <- NA
}
if(length(corrected) > 0){ DF$Corrected.Items[i] <- paste(corrected, collapse = " ") } else {
DF$Corrected.Items[i] <- NA
}
if(length(extras_final) > 0){ DF$Extra.Items[i] <- paste(extras_final, collapse = " ") } else {
DF$Extra.Items[i] <- NA
}
DF$Proportion.Match[i] <- (length(shared) + length(corrected)) / length(answer.tokens)
}
#Create blank containers for the output
s_match = c()
View(DF)
#create participant summary ----
k <- tapply(DF$Trial.ID, DF$Sub.ID, length)
if(min(k) != max(k)){
warning("The number of trials is not the same for every participant.
We will use the max value of trials to calculate proportion
correct. Check your data if this is not intended.")
}
k <- max(k)
k
#create participant data frame ----
DF_participant <- aggregate(DF$Scored, list(DF$Sub.ID), function(x){sum(x)/k})
names(DF)
#create participant data frame ----
DF_participant <- aggregate(DF$Proportion.Match, list(DF$Sub.ID), mean)
DF_part
DF_participant
colnames(DF_participant) <- c("Sub.ID", "Proportion.Correct")
if (!is.null(other)){
DF_participant <- merge(DF_participant, other_unique, by = "Sub.ID")
}
if (!is.null(group.by)){
DF_participant <- merge(DF_participant, group.by_unique, by = "Sub.ID")
}
DF_participant
#if they want to flag participants ----
if (flag) {
#flag by group
if (!is.null(group.by)){
DF_participant$Z.Score.Group <- ave(DF_participant$Proportion.Correct,
DF_participant[ , colnames(group.by)[1:ncol(group.by)-1]],
FUN = scale)
}
DF_participant$Z.Score.Participant <- scale(DF_participant$Proportion.Correct)
}
DF_participant
#group summary ----
#if they want a grouping variable
if (!is.null(group.by)){
#summarize participant scores by group
DF_group_person <- aggregate(DF$Scored,
list(DF[ , colnames(group.by)[1:ncol(group.by)-1]],
DF$Sub.ID),
function(x){sum(x)/k})
colnames(DF_group_person) <- c(colnames(group.by)[1:ncol(group.by)-1],
"Sub.ID", "Mean")
DF_group <- aggregate(DF_group_person$Mean,
list(DF_group_person[ , colnames(group.by)[1:ncol(group.by)-1]]), mean)
DF_group$SD <- aggregate(DF_group_person$Mean,
list(DF_group_person[ , colnames(group.by)[1:ncol(group.by)-1]]), sd)$x
DF_group$N <- aggregate(DF_group_person$Mean,
list(DF_group_person[ , colnames(group.by)[1:ncol(group.by)-1]]), length)$x
DF_group <- rbind(DF_group,
c("overall", mean(DF_group_person$Mean),
sd(DF_group_person$Mean), length(DF_group_person$Mean)))
colnames(DF_group) <- c(colnames(group.by)[1:ncol(group.by)-1], "Mean", "SD", "N")
return(list(DF_Scored = DF,
DF_Participant = DF_participant,
DF_Group = DF_group))
} else {
return(list(DF_Scored = DF,
DF_Participant = DF_participant))
}
#summarize participant scores by group
DF_group_person <- aggregate(DF$Proportion.Match,
list(DF[ , colnames(group.by)[1:ncol(group.by)-1]],
DF$Sub.ID),
function(x){sum(x)/k})
colnames(DF_group_person) <- c(colnames(group.by)[1:ncol(group.by)-1],
"Sub.ID", "Mean")
DF_group <- aggregate(DF_group_person$Mean,
list(DF_group_person[ , colnames(group.by)[1:ncol(group.by)-1]]), mean)
DF_group$SD <- aggregate(DF_group_person$Mean,
list(DF_group_person[ , colnames(group.by)[1:ncol(group.by)-1]]), sd)$x
DF_group$N <- aggregate(DF_group_person$Mean,
list(DF_group_person[ , colnames(group.by)[1:ncol(group.by)-1]]), length)$x
DF_group <- rbind(DF_group,
c("overall", mean(DF_group_person$Mean),
sd(DF_group_person$Mean), length(DF_group_person$Mean)))
colnames(DF_group) <- c(colnames(group.by)[1:ncol(group.by)-1], "Mean", "SD", "N")
DF_group
DF_test <- read.csv("data/sentence_data.csv")
source("R/prop_correct_sentence.R")
source("R/prop_correct_sentence.R")
scored_output <- prop_correct_sentence(
responses = DF_test$Response,
key = DF_test$Sentence,
key.trial = DF_test$Trial.ID,
id = DF_test$Sub.ID,
id.trial = DF_test$Trial.ID,
cutoff = 1,
flag = TRUE,
group.by = DF_test$Condition,
group.by.names = "condition",
other = c(rep("stuff", nrow(DF_test))),
other.names = "fake_column",
token.split = " "
)
source("R/prop_correct_sentence.R")
scored_output <- prop_correct_sentence(
responses = DF_test$Response,
key = DF_test$Sentence,
key.trial = DF_test$Trial.ID,
id = DF_test$Sub.ID,
id.trial = DF_test$Trial.ID,
cutoff = 1,
flag = TRUE,
group.by = DF_test$Condition,
group.by.names = "condition",
other = c(rep("stuff", nrow(DF_test))),
other.names = "fake_column",
token.split = " "
)
scored_output$DF_Scored
scored_output$DF_Participant
scored_output$DF_Group
